{"paragraphs":[{"text":"import org.apache.spark.ml.linalg.{DenseVector, Vector, Vectors}\nimport org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.regression.{GBTRegressionModel, GBTRegressor}\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.SaveMode\n\nimport spark.implicits._\n\n// to install dmlc package in docker\n// import ml.dmlc.xgboost4j.scala.spark.XGBoostRegressor\n","user":"anonymous","dateUpdated":"2019-12-16T21:45:40+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.ml.linalg.{DenseVector, Vector, Vectors}\nimport org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.regression.{GBTRegressionModel, GBTRegressor}\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.SaveMode\nimport spark.implicits._\n"}]},"apps":[],"jobName":"paragraph_1575580982612_2123006517","id":"20191205-212302_1535408969","dateCreated":"2019-12-05T21:23:02+0000","dateStarted":"2019-12-16T21:45:41+0000","dateFinished":"2019-12-16T21:45:41+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:90783"},{"text":"\n  val arrayToVecUDF = udf{ (x: Seq[Double]) => Vectors.dense(x.toArray) }\n\n  val trainData = spark.read.load(s\"s3a://strava.scratch/gbm/bay-area/etaTrainingData/part-0000[012]*.parquet\").\n  withColumn(\"features\", arrayToVecUDF(col(\"featuresArray\"))).drop(\"featuresArray\")\n  \n  val valData = spark.read.load(s\"s3a://strava.scratch/gbm/bay-area/etaValidationData\").\n    withColumn(\"features\", arrayToVecUDF(col(\"featuresArray\"))).drop(\"featuresArray\")\n\n  \n  trainData.printSchema","user":"anonymous","dateUpdated":"2019-12-06T00:00:22+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"arrayToVecUDF: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7,Some(List(ArrayType(DoubleType,false))))\ntrainData: org.apache.spark.sql.DataFrame = [edgeUID: bigint, athleteId: bigint ... 3 more fields]\nvalData: org.apache.spark.sql.DataFrame = [edgeUID: bigint, athleteId: bigint ... 3 more fields]\nroot\n |-- edgeUID: long (nullable = true)\n |-- athleteId: long (nullable = true)\n |-- is_best_effort: double (nullable = true)\n |-- label: double (nullable = true)\n |-- features: vector (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1575580992385_-2103837640","id":"20191205-212312_1461850965","dateCreated":"2019-12-05T21:23:12+0000","dateStarted":"2019-12-06T00:00:22+0000","dateFinished":"2019-12-06T00:00:24+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:90784"},{"text":"  def getPipeline(): Pipeline = {\n    val gbt = new GBTRegressor().\n      setLabelCol(\"label\").\n      setFeaturesCol(\"features\").\n      setMaxDepth(10).\n      setMaxIter(300).\n      setSubsamplingRate(0.7).\n      setLossType(\"squared\").\n      setPredictionCol(\"ETA\").\n      setStepSize(0.05)\n\n    new Pipeline().setStages(Array(gbt))\n  }\n  \n  val modelPipeline = getPipeline.fit(trainData)","user":"anonymous","dateUpdated":"2019-12-06T00:01:31+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"getPipeline: ()org.apache.spark.ml.Pipeline\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 11 in stage 9415.0 failed 4 times, most recent failure: Lost task 11.3 in stage 9415.0 (TID 329354, 10.0.22.251, executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\n  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n  at scala.Option.foreach(Option.scala:257)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n  at org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:743)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:742)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n  at org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:742)\n  at org.apache.spark.ml.tree.impl.RandomForest$.findSplitsBySorting(RandomForest.scala:927)\n  at org.apache.spark.ml.tree.impl.RandomForest$.findSplits(RandomForest.scala:904)\n  at org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:121)\n  at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$2.apply(DecisionTreeRegressor.scala:129)\n  at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$2.apply(DecisionTreeRegressor.scala:124)\n  at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:183)\n  at scala.util.Try$.apply(Try.scala:192)\n  at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:183)\n  at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:124)\n  at org.apache.spark.ml.tree.impl.GradientBoostedTrees$.boost(GradientBoostedTrees.scala:330)\n  at org.apache.spark.ml.tree.impl.GradientBoostedTrees$.run(GradientBoostedTrees.scala:50)\n  at org.apache.spark.ml.regression.GBTRegressor$$anonfun$train$1.apply(GBTRegressor.scala:183)\n  at org.apache.spark.ml.regression.GBTRegressor$$anonfun$train$1.apply(GBTRegressor.scala:155)\n  at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:183)\n  at scala.util.Try$.apply(Try.scala:192)\n  at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:183)\n  at org.apache.spark.ml.regression.GBTRegressor.train(GBTRegressor.scala:155)\n  at org.apache.spark.ml.regression.GBTRegressor.train(GBTRegressor.scala:59)\n  at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n  at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)\n  at org.apache.spark.ml.Pipeline$$anonfun$fit$2.apply(Pipeline.scala:153)\n  at org.apache.spark.ml.Pipeline$$anonfun$fit$2.apply(Pipeline.scala:149)\n  at scala.collection.Iterator$class.foreach(Iterator.scala:742)\n  at scala.collection.AbstractIterator.foreach(Iterator.scala:1194)\n  at scala.collection.IterableViewLike$Transformed$class.foreach(IterableViewLike.scala:44)\n  at scala.collection.SeqViewLike$AbstractTransformed.foreach(SeqViewLike.scala:37)\n  at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:149)\n  at $line45173894572.$read$<init>(<console>:40)\n  at $line45173894572.$read$<init>(<console>:45)\n  at $line45173894572.$read$<init>(<console>:47)\n  at $line45173894572.$read$<init>(<console>:49)\n  at $line45173894572.$read$<init>(<console>:51)\n  at $line45173894572.$read$<init>(<console>:53)\n  at $line45173894572.$read$<init>(<console>:55)\n  at $line45173894572.$read$<init>(<console>:57)\n  at $line45173894572.$read$<init>(<console>:59)\n  at $line45173894572.$read$<init>(<console>:61)\n  at $line45173894572.$read$<init>(<console>:63)\n  at $line45173894572.$read.<init>(<console>:65)\n  at $line45173894572.$read$.<init>(<console>:69)\n  at $line45173894572.$read$.<clinit>(<console>)\n  at $line45173894572.$eval$.$print$lzycompute(<console>:7)\n  at $line45173894572.$eval$.$print(<console>:6)\n  at $line45173894572.$eval.$print(<console>)\n  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n  at java.lang.reflect.Method.invoke(Method.java:498)\n  at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:784)\n  at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1039)\n  at scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:636)\n  at scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:635)\n  at scala.reflect.internal.util.ScalaClassLoader$class.asContext(ScalaClassLoader.scala:31)\n  at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:19)\n  at scala.tools.nsc.interpreter.IMain$WrappedRequest.loadAndRunReq(IMain.scala:635)\n  at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:567)\n  at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:563)\n  at sun.reflect.GeneratedMethodAccessor95.invoke(Unknown Source)\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n  at java.lang.reflect.Method.invoke(Method.java:498)\n  at org.apache.zeppelin.spark.Utils.invokeMethod(Utils.java:44)\n  at org.apache.zeppelin.spark.OldSparkInterpreter.interpret(OldSparkInterpreter.java:907)\n  at org.apache.zeppelin.spark.OldSparkInterpreter.interpretInput(OldSparkInterpreter.java:1155)\n  at org.apache.zeppelin.spark.OldSparkInterpreter.interpret(OldSparkInterpreter.java:1101)\n  at org.apache.zeppelin.spark.OldSparkInterpreter.interpret(OldSparkInterpreter.java:1092)\n  at org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:73)\n  at org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:103)\n  at org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:633)\n  at org.apache.zeppelin.scheduler.Job.run(Job.java:188)\n  at org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:140)\n  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n  at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n  at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n  at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n  at java.lang.Thread.run(Thread.java:748)\n"}]},"apps":[],"jobName":"paragraph_1575583962444_1871089263","id":"20191205-221242_234646968","dateCreated":"2019-12-05T22:12:42+0000","dateStarted":"2019-12-06T00:01:31+0000","dateFinished":"2019-12-06T02:07:18+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:90785"},{"text":"// val path = \"s3a://strava-mlmodels/routeETA/toyModel\"\nval path = s\"s3a://strava.scratch/gbm/bay-area/etaModel/version0.0\"\nmodelPipeline.write.overwrite().save(path)\n","user":"anonymous","dateUpdated":"2019-12-06T00:04:43+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"path: String = s3a://strava.scratch/gbm/bay-area/etaModel/version0.0\n"}]},"apps":[],"jobName":"paragraph_1575586567525_-625630983","id":"20191205-225607_1320327919","dateCreated":"2019-12-05T22:56:07+0000","dateStarted":"2019-12-06T00:04:43+0000","dateFinished":"2019-12-06T02:07:38+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:90786"},{"text":"val trainResult = modelPipeline.transform(trainData).drop(\"features\")\nval valResult = modelPipeline.transform(valData).drop(\"features\")","user":"anonymous","dateUpdated":"2019-12-06T00:05:22+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"trainResult: org.apache.spark.sql.DataFrame = [edgeUID: bigint, athleteId: bigint ... 3 more fields]\nvalResult: org.apache.spark.sql.DataFrame = [edgeUID: bigint, athleteId: bigint ... 3 more fields]\n"}]},"apps":[],"jobName":"paragraph_1575586514556_765566308","id":"20191205-225514_903215923","dateCreated":"2019-12-05T22:55:14+0000","dateStarted":"2019-12-06T02:07:18+0000","dateFinished":"2019-12-06T02:07:39+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:90787"},{"text":"trainResult.repartition(1).write.mode(SaveMode.Overwrite).option(\"compression\", \"gzip\").parquet(\"s3a://strava.scratch/gbm/bay-area/etaResult/version0.0/train\")\nvalResult.repartition(1).write.mode(SaveMode.Overwrite).option(\"compression\", \"gzip\").parquet(\"s3a://strava.scratch/gbm/bay-area/etaResult/version0.0/val\")","user":"anonymous","dateUpdated":"2019-12-06T00:05:25+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1575587783788_-154331300","id":"20191205-231623_2071117536","dateCreated":"2019-12-05T23:16:23+0000","dateStarted":"2019-12-06T02:07:38+0000","dateFinished":"2019-12-06T02:09:01+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:90788"},{"text":"// plot distribution of error, grouped by is\nval trainResult = spark.read.load(\"s3a://strava.scratch/gbm/bay-area/etaResult/version0.0/train\")\ntrainResult.printSchema\ntrainResult.createOrReplaceTempView(\"train\")\n\nval valResult = spark.read.load(\"s3a://strava.scratch/gbm/bay-area/etaResult/version0.0/val\")\nvalResult.createOrReplaceTempView(\"val\")\n\n","user":"anonymous","dateUpdated":"2019-12-06T22:34:18+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"trainResult: org.apache.spark.sql.DataFrame = [edgeUID: bigint, athleteId: bigint ... 3 more fields]\nroot\n |-- edgeUID: long (nullable = true)\n |-- athleteId: long (nullable = true)\n |-- is_best_effort: double (nullable = true)\n |-- label: double (nullable = true)\n |-- ETA: double (nullable = true)\n\nvalResult: org.apache.spark.sql.DataFrame = [edgeUID: bigint, athleteId: bigint ... 3 more fields]\nmySql: String =\n\"\nselect\n(ETA - label) as err, (ETA - label) * 100.0 / label as errPercent, ETA, label\nfrom train\nwhere is_best_effort < 0.5\n\"\ndf: org.apache.spark.sql.DataFrame = [err: double, errPercent: double ... 2 more fields]\nres48: df.type = [err: double, errPercent: double ... 2 more fields]\n"}]},"apps":[],"jobName":"paragraph_1575587876195_-5544122","id":"20191205-231756_1599132393","dateCreated":"2019-12-05T23:17:56+0000","dateStarted":"2019-12-06T21:34:02+0000","dateFinished":"2019-12-06T21:34:03+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:90789"},{"text":"val mySql = \"\"\"\nselect\n(ETA - label) as err, (ETA - label) * 100.0 / label as errPercent\nfrom val\nwhere is_best_effort > 0.5\n\"\"\"\n\nval df = spark.sql(mySql)\ndf.cache()","user":"anonymous","dateUpdated":"2019-12-06T23:04:48+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"mySql: String =\n\"\nselect\n(ETA - label) as err, (ETA - label) * 100.0 / label as errPercent\nfrom val\nwhere is_best_effort > 0.5\n\"\ndf: org.apache.spark.sql.DataFrame = [err: double, errPercent: double]\nres56: df.type = [err: double, errPercent: double]\n"}]},"apps":[],"jobName":"paragraph_1575671659761_-1045469676","id":"20191206-223419_1979106830","dateCreated":"2019-12-06T22:34:19+0000","dateStarted":"2019-12-06T23:04:48+0000","dateFinished":"2019-12-06T23:04:49+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:90790"},{"text":"\nval mySql = \"\"\"\nselect\napprox_percentile(abs(ETA - label), 0.05) as p5,\napprox_percentile(abs(ETA - label), 0.1) as p10,\napprox_percentile(abs(ETA - label), 0.5) as p50,\napprox_percentile(abs(ETA - label), 0.9) as p90,\napprox_percentile(abs(ETA - label), 0.95) as p95\n\nfrom val\nwhere is_best_effort > 0.5\n\"\"\"\n\nspark.sql(mySql).collect()","user":"anonymous","dateUpdated":"2019-12-06T23:05:17+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"mySql: String =\n\"\nselect\napprox_percentile(abs(ETA - label), 0.05) as p5,\napprox_percentile(abs(ETA - label), 0.1) as p10,\napprox_percentile(abs(ETA - label), 0.5) as p50,\napprox_percentile(abs(ETA - label), 0.9) as p90,\napprox_percentile(abs(ETA - label), 0.95) as p95\n\nfrom val\nwhere is_best_effort > 0.5\n\"\nres58: Array[org.apache.spark.sql.Row] = Array([0.08236222054600573,0.16517139575606166,1.1800190320012245,6.851942438548889,11.65738737247337])\n"}]},"apps":[],"jobName":"paragraph_1575671260407_490334057","id":"20191206-222740_308512482","dateCreated":"2019-12-06T22:27:40+0000","dateStarted":"2019-12-06T23:05:17+0000","dateFinished":"2019-12-06T23:05:19+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:90791"},{"text":"df.count","user":"anonymous","dateUpdated":"2019-12-06T23:06:43+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res59: Long = 554229\n"}]},"apps":[],"jobName":"paragraph_1575671707564_936839357","id":"20191206-223507_157516253","dateCreated":"2019-12-06T22:35:07+0000","dateStarted":"2019-12-06T23:06:43+0000","dateFinished":"2019-12-06T23:06:44+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:90792"},{"text":"\n\nval errDistribution = df.select(\"err\").filter(abs(col(\"err\")).lt(100)).as[Double].rdd.histogram(20)\n","user":"anonymous","dateUpdated":"2019-12-06T23:15:15+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"errDistribution: (Array[Double], Array[Long]) = (Array(-99.52228282866434, -89.55590664587794, -79.58953046309152, -69.62315428030512, -59.6567780975187, -49.69040191473229, -39.72402573194588, -29.75764954915948, -19.791273366373062, -9.824897183586643, 0.14147899919976226, 10.107855181986167, 20.074231364772587, 30.040607547559006, 40.00698373034538, 49.973359913131816, 59.93973609591822, 69.90611227870465, 79.87248846149106, 89.83886464427746, 99.80524082706386),Array(69, 104, 151, 227, 376, 622, 1268, 2920, 10566, 277540, 242552, 10568, 3464, 1612, 852, 460, 249, 151, 109, 69))\n"}]},"apps":[],"jobName":"paragraph_1575651976704_-1334746183","id":"20191206-170616_1478257511","dateCreated":"2019-12-06T17:06:16+0000","dateStarted":"2019-12-06T23:15:15+0000","dateFinished":"2019-12-06T23:15:16+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:90793"},{"text":"val errPercentDistribution = df.select(\"errPercent\").filter(abs(col(\"errPercent\")).lt(200)).as[Double].rdd.histogram(20)\n","user":"anonymous","dateUpdated":"2019-12-06T23:15:19+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"errPercentDistribution: (Array[Double], Array[Long]) = (Array(-193.77955500404278, -174.09293100261604, -154.4063070011893, -134.7196829997626, -115.03305899833586, -95.34643499690914, -75.65981099548242, -55.97318699405568, -36.286562992628944, -16.599938991202237, 3.0866850102244996, 22.773309011651236, 42.45993301307794, 62.14655701450471, 81.83318101593142, 101.51980501735818, 121.20642901878489, 140.8930530202116, 160.5796770216383, 180.266301023065, 199.95292502449178),Array(12, 11, 19, 26, 48, 202, 1437, 10135, 71481, 224896, 157003, 48521, 17545, 8367, 4627, 2772, 1822, 1260, 931, 700))\n"}]},"apps":[],"jobName":"paragraph_1575668921178_2122889713","id":"20191206-214841_1610429413","dateCreated":"2019-12-06T21:48:41+0000","dateStarted":"2019-12-06T23:15:19+0000","dateFinished":"2019-12-06T23:15:20+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:90794"},{"text":"val labelDistribution = df.select(\"label\").as[Double].rdd.histogram(20)","user":"anonymous","dateUpdated":"2019-12-06T22:45:32+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"labelDistribution: (Array[Double], Array[Long]) = (Array(1.0000016647109078, 25.074568413321323, 49.14913516193174, 73.22370191054215, 97.29826865915257, 121.37283540776298, 145.44740215637339, 169.5219689049838, 193.59653565359423, 217.67110240220464, 241.74566915081505, 265.8202358994255, 289.89480264803586, 313.9693693966463, 338.0439361452567, 362.1185028938671, 386.19306964247755, 410.26763639108793, 434.34220313969837, 458.41676988830875, 482.4913366369192),Array(1093851, 249203, 69977, 30393, 17137, 10481, 6304, 4089, 2997, 2251, 1712, 1356, 1011, 802, 613, 452, 343, 216, 127, 43))\n"}]},"apps":[],"jobName":"paragraph_1575652642314_-602643330","id":"20191206-171722_748854129","dateCreated":"2019-12-06T17:17:22+0000","dateStarted":"2019-12-06T22:45:32+0000","dateFinished":"2019-12-06T22:45:33+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:90795"},{"text":"val buckets = Range(0, 501, 20).toArray.map(_.toDouble)\nval ETADistribution = df.select(\"ETA\").filter(col(\"ETA\").gt(0)).as[Double].rdd.histogram(buckets, true)\n","user":"anonymous","dateUpdated":"2019-12-06T22:45:48+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"buckets: Array[Double] = Array(0.0, 20.0, 40.0, 60.0, 80.0, 100.0, 120.0, 140.0, 160.0, 180.0, 200.0, 220.0, 240.0, 260.0, 280.0, 300.0, 320.0, 340.0, 360.0, 380.0, 400.0, 420.0, 440.0, 460.0, 480.0, 500.0)\nETADistribution: Array[Long] = Array(978986, 304634, 100374, 39892, 20072, 13510, 10883, 6925, 4347, 2668, 1812, 1488, 1443, 1652, 1447, 935, 600, 377, 329, 248, 184, 113, 62, 20, 3)\n"}]},"apps":[],"jobName":"paragraph_1575658497879_282134448","id":"20191206-185457_879970231","dateCreated":"2019-12-06T18:54:57+0000","dateStarted":"2019-12-06T22:45:48+0000","dateFinished":"2019-12-06T22:45:49+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:90796"},{"text":"val labelDistribution = df.select(\"label\").as[Double].rdd.histogram(buckets, true)","user":"anonymous","dateUpdated":"2019-12-06T22:45:52+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"labelDistribution: Array[Long] = Array(985553, 297360, 100519, 40267, 22157, 13917, 9340, 6040, 4168, 2984, 2368, 1864, 1499, 1177, 1019, 748, 644, 507, 379, 335, 213, 166, 93, 38, 3)\n"}]},"apps":[],"jobName":"paragraph_1575658508438_-441110651","id":"20191206-185508_1625143069","dateCreated":"2019-12-06T18:55:08+0000","dateStarted":"2019-12-06T22:45:52+0000","dateFinished":"2019-12-06T22:45:52+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:90797"},{"text":"// copy over edge feature to \nimport org.apache.spark.sql.{DataFrame, SaveMode}\n\nval df = spark.read.load(\"s3a://strava.scratch/gbm/bay-area/edgeFeatureArray\")\ndf.write.mode(SaveMode.Overwrite).option(\"compression\", \"gzip\").parquet(s\"s3a://strava-mlmodels/routeETA/edgeFeatureArray\")","user":"anonymous","dateUpdated":"2019-12-16T18:40:19+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.{DataFrame, SaveMode}\ndf: org.apache.spark.sql.DataFrame = [edgeUID: bigint, featuresArray: array<double>]\njava.nio.file.AccessDeniedException: s3a://strava-mlmodels/routeETA/edgeFeatureArray: getFileStatus on s3a://strava-mlmodels/routeETA/edgeFeatureArray: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: 20C576759EB5F133; S3 Extended Request ID: vrdqxqr9YOxmxDbHJNwyNy1c7NqsB6pbZb+QjwLeZWEQhIHgaBDSLUB8NTk3IN09tqFx2cq3qZQ=), S3 Extended Request ID: vrdqxqr9YOxmxDbHJNwyNy1c7NqsB6pbZb+QjwLeZWEQhIHgaBDSLUB8NTk3IN09tqFx2cq3qZQ=\n  at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:174)\n  at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:117)\n  at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:1856)\n  at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:1822)\n  at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1763)\n  at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1627)\n  at org.apache.hadoop.fs.s3a.S3AFileSystem.exists(S3AFileSystem.java:2500)\n  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:93)\n  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n  at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\n  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\n  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:668)\n  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:276)\n  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:270)\n  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:228)\n  at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:557)\n  ... 46 elided\nCaused by: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: 20C576759EB5F133; S3 Extended Request ID: vrdqxqr9YOxmxDbHJNwyNy1c7NqsB6pbZb+QjwLeZWEQhIHgaBDSLUB8NTk3IN09tqFx2cq3qZQ=)\n  at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1639)\n  at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1304)\n  at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1056)\n  at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:743)\n  at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:717)\n  at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:699)\n  at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:667)\n  at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:649)\n  at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:513)\n  at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4325)\n  at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4272)\n  at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1264)\n  at org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:1053)\n  at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:1841)\n  ... 72 more\n"}]},"apps":[],"jobName":"paragraph_1575668472478_-1356914851","id":"20191206-214112_1318132837","dateCreated":"2019-12-06T21:41:12+0000","dateStarted":"2019-12-16T18:40:19+0000","dateFinished":"2019-12-16T18:40:44+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:90798"},{"text":"val test = spark.read.load(\"s3a://strava-mlmodels/demo/xgboost-churn/train/train.csv\")","user":"anonymous","dateUpdated":"2019-12-19T18:55:29+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 625.0 failed 4 times, most recent failure: Lost task 0.3 in stage 625.0 (TID 112363, 10.0.27.77, executor 1): org.apache.spark.SparkException: Exception thrown in awaitResult:\n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)\n\tat org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:290)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:538)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$9.apply(ParquetFileFormat.scala:611)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$9.apply(ParquetFileFormat.scala:603)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.nio.file.AccessDeniedException: s3a://strava-mlmodels/demo/xgboost-churn/train/train.csv: getFileStatus on s3a://strava-mlmodels/demo/xgboost-churn/train/train.csv: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: 798F92B5BD763DDB; S3 Extended Request ID: art90UvRLBFAylT88LFzm+xkZPed6rn5LRRHY7A5J/00YdgmbiE0peorJ0y8efGRj1JJ31enlHw=), S3 Extended Request ID: art90UvRLBFAylT88LFzm+xkZPed6rn5LRRHY7A5J/00YdgmbiE0peorJ0y8efGRj1JJ31enlHw=\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:174)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:117)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:1856)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:1822)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1763)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:585)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:914)\n\tat org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:65)\n\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:498)\n\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:476)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$readParquetFootersInParallel$1.apply(ParquetFileFormat.scala:544)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$readParquetFootersInParallel$1.apply(ParquetFileFormat.scala:538)\n\tat org.apache.spark.util.ThreadUtils$$anonfun$3$$anonfun$apply$1.apply(ThreadUtils.scala:287)\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n\tat scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala:121)\n\tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n\tat scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n\tat scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n\tat scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\nCaused by: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: 798F92B5BD763DDB; S3 Extended Request ID: art90UvRLBFAylT88LFzm+xkZPed6rn5LRRHY7A5J/00YdgmbiE0peorJ0y8efGRj1JJ31enlHw=), S3 Extended Request ID: art90UvRLBFAylT88LFzm+xkZPed6rn5LRRHY7A5J/00YdgmbiE0peorJ0y8efGRj1JJ31enlHw=\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1639)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1304)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1056)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:743)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:717)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:699)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:667)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:649)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:513)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4325)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4272)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1264)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:1053)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:1841)\n\t... 17 more\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\n  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n  at scala.Option.foreach(Option.scala:257)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n  at org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n  at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.mergeSchemasInParallel(ParquetFileFormat.scala:633)\n  at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.inferSchema(ParquetFileFormat.scala:241)\n  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$6.apply(DataSource.scala:180)\n  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$6.apply(DataSource.scala:180)\n  at scala.Option.orElse(Option.scala:289)\n  at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:179)\n  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:373)\n  at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\n  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178)\n  ... 46 elided\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult:\n  at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)\n  at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:290)\n  at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:538)\n  at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$9.apply(ParquetFileFormat.scala:611)\n  at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$9.apply(ParquetFileFormat.scala:603)\n  at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\n  at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\n  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n  at org.apache.spark.scheduler.Task.run(Task.scala:121)\n  at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n  ... 3 more\nCaused by: java.nio.file.AccessDeniedException: s3a://strava-mlmodels/demo/xgboost-churn/train/train.csv: getFileStatus on s3a://strava-mlmodels/demo/xgboost-churn/train/train.csv: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: 798F92B5BD763DDB; S3 Extended Request ID: art90UvRLBFAylT88LFzm+xkZPed6rn5LRRHY7A5J/00YdgmbiE0peorJ0y8efGRj1JJ31enlHw=), S3 Extended Request ID: art90UvRLBFAylT88LFzm+xkZPed6rn5LRRHY7A5J/00YdgmbiE0peorJ0y8efGRj1JJ31enlHw=\n  at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:174)\n  at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:117)\n  at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:1856)\n  at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:1822)\n  at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1763)\n  at org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:585)\n  at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:914)\n  at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:65)\n  at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:498)\n  at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:476)\n  at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$readParquetFootersInParallel$1.apply(ParquetFileFormat.scala:544)\n  at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$readParquetFootersInParallel$1.apply(ParquetFileFormat.scala:538)\n  at org.apache.spark.util.ThreadUtils$$anonfun$3$$anonfun$apply$1.apply(ThreadUtils.scala:287)\n  at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n  at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n  at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala:121)\n  at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n  at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n  at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n  at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\nCaused by: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: 798F92B5BD763DDB; S3 Extended Request ID: art90UvRLBFAylT88LFzm+xkZPed6rn5LRRHY7A5J/00YdgmbiE0peorJ0y8efGRj1JJ31enlHw=)\n  at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1639)\n  at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1304)\n  at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1056)\n  at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:743)\n  at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:717)\n  at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:699)\n  at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:667)\n  at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:649)\n  at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:513)\n  at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4325)\n  at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4272)\n  at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1264)\n  at org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:1053)\n  at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:1841)\n  ... 17 more\n"}]},"apps":[],"jobName":"paragraph_1576521619308_817972231","id":"20191216-184019_1728632839","dateCreated":"2019-12-16T18:40:19+0000","dateStarted":"2019-12-19T18:55:29+0000","dateFinished":"2019-12-19T18:55:30+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:90799"},{"text":"df.printSchema","user":"anonymous","dateUpdated":"2019-12-16T18:55:39+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- edgeUID: long (nullable = true)\n |-- featuresArray: array (nullable = true)\n |    |-- element: double (containsNull = true)\n\n"}]},"apps":[],"jobName":"paragraph_1576521693997_-505055370","id":"20191216-184133_799395547","dateCreated":"2019-12-16T18:41:33+0000","dateStarted":"2019-12-16T18:55:39+0000","dateFinished":"2019-12-16T18:55:39+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:90800"},{"text":"","user":"anonymous","dateUpdated":"2019-12-16T22:40:41+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res3: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@1618bd7c\n"}]},"apps":[],"jobName":"paragraph_1576535804625_-1226912511","id":"20191216-223644_2096779100","dateCreated":"2019-12-16T22:36:44+0000","dateStarted":"2019-12-16T22:40:22+0000","dateFinished":"2019-12-16T22:40:22+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:90801"},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1576536022668_-1616502527","id":"20191216-224022_1112342172","dateCreated":"2019-12-16T22:40:22+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:90802"}],"name":"lucindaz/edgeLevelModel","id":"2EUA8QP4M","noteParams":{},"noteForms":{},"angularObjects":{"snowflake:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}